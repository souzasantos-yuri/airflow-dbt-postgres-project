# ðŸ“Š Airflow + dbt + PostgreSQL Project

This project demonstrates how to build a modern **data pipeline** using:

- **Apache Airflow** â€” workflow orchestration  
- **dbt (Data Build Tool)** â€” data transformation and modeling  
- **PostgreSQL** â€” data warehouse / database  
- **Docker & Docker Compose** â€” containerized environment  

The goal of this project is to implement an automated **ELT pipeline** that:

1. ðŸ“¥ Extracts data  
2. ðŸ—„ Loads data into PostgreSQL  
3. ðŸ”„ Transforms data using dbt  
4. âš™ Orchestrates everything using Airflow  

---

## ðŸš€ Tech Stack

- Python  
- Apache Airflow  
- dbt  
- PostgreSQL  
- Docker & Docker Compose  
- Bash / Shell  

---
        +------------+
        |   Data     |
        |  Source    |
        +------------+
               â†“
        +------------+
        | PostgreSQL |
        |   (Raw)    |
        +------------+
               â†“
        +------------+
        |    dbt     |
        | Transform  |
        +------------+
               â†“
        +------------+
        | PostgreSQL |
        |   (Mart)   |
        +------------+
               â†‘
        +------------+
        |  Airflow   |
        | Orchestrator|
        +------------+

---

## âœ… Best Practices Applied

- Containerized environment with Docker  
- Separation of concerns (orchestration vs transformation)  
- Layered modeling approach (staging â†’ marts)  
- Environment-based configuration  
- Version control with Git  
- Data quality testing with dbt  
